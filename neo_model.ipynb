{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Import the data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (balanced_accuracy_score,\n",
    "                             classification_report, confusion_matrix)\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the base dataframe from which the analysis will take place.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the JSON file\n",
    "path = 'resources/neo_data.json'\n",
    "\n",
    "df_original = pd.read_json(path)\n",
    "df_original.info()\n",
    "\n",
    "def remove_unnecessary_columns(df):\n",
    "    '''\n",
    "        Drop the columns that serve to identify the Near Earth Object, which are\n",
    "        assigned by NASA and not scientifically descriptive of the object itself.\n",
    "        Return a dataframe without the unnecessary columns.\n",
    "    '''\n",
    "    # Create a new DataFrame\n",
    "    df_columns_removed = df.copy()\n",
    "    superfluous_columns = ['name', 'id', 'orbiting_body']\n",
    "    # Drop the superfluous columns\n",
    "    df_columns_removed.drop(superfluous_columns, axis=1, inplace=True)\n",
    "    return df_columns_removed\n",
    "\n",
    "base_df = remove_unnecessary_columns(df_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminary model experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_set(df):\n",
    "    X = df.copy()\n",
    "    # Drop the target column\n",
    "    X.drop('is_potentially_hazardous', axis=1, inplace=True)\n",
    "    return X\n",
    "\n",
    "# Define the features set and drop the target column\n",
    "X = get_features_set(base_df)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target vector\n",
    "y = base_df['is_potentially_hazardous'].copy()\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check value_counts\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No testing has been done yet to determine the data is distrubuted normally.\n",
    "# We will use both the StandardScaler and the MinMaxScaler to determine which\n",
    "# one is best for this dataset. Let's start with the StandardScaler.\n",
    "standard_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = standard_scaler.transform(X_train)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the testing dataset\n",
    "X_test_scaled = standard_scaler.transform(X_test)\n",
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the max and min of the scaled training and testing sets\n",
    "print(\"Scaled data min/max (StandardScaler):\")\n",
    "print(\"Training data min:\",X_train_scaled.min())\n",
    "print(\"Training data max:\",X_train_scaled.max())\n",
    "print(\"Testing data min:\",X_test_scaled.min())\n",
    "print(\"Testing data max:\",X_test_scaled.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_minmax = minmax_scaler.transform(X_train)\n",
    "X_train_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_minmax = minmax_scaler.transform(X_test)\n",
    "X_test_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the max and min of the scaled training and testing sets\n",
    "print(\"Scaled data min/max (MinMaxScaler):\")\n",
    "print(\"Training data min:\",X_train_minmax.min())\n",
    "print(\"Training data max:\",X_train_minmax.max())\n",
    "print(\"Testing data min:\",X_test_minmax.min())\n",
    "print(\"Testing data max:\",X_test_minmax.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN and DecisionTree models were created on a separate file and showed accuracy scores of 1.0, demonstrating that the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_standard = SVC(kernel='rbf')\n",
    "svc_standard.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model by checking the model accuracy with model.score\n",
    "print('Train Accuracy: %.3f' % svc_standard.score(X_train_scaled, y_train))\n",
    "print('Test Accuracy: %.3f' % svc_standard.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_minmax = SVC(kernel='rbf')\n",
    "svc_minmax.fit(X_train_minmax, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the model by checking the model accuracy with model.score\n",
    "print('Train Accuracy: %.3f' % svc_minmax.score(X_train_minmax, y_train))\n",
    "print('Test Accuracy: %.3f' % svc_minmax.score(X_test_minmax, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loop to vary the max_depth parameter\n",
    "# Make sure to record the train and test scores \n",
    "# for each pass.\n",
    "\n",
    "# Depths should span from 1 up to 40 in steps of 2\n",
    "depths = range(1, 10, 1)\n",
    "\n",
    "# The scores dataframe will hold depths and scores\n",
    "# to make plotting easy\n",
    "scores = {'train': [], 'test': [], 'depth': []}\n",
    "\n",
    "# Loop through each depth (this will take time to run)\n",
    "for depth in depths:\n",
    "    clf = RandomForestClassifier(max_depth=depth)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    train_score = clf.score(X_train_scaled, y_train)\n",
    "    test_score = clf.score(X_test_scaled, y_test)\n",
    "\n",
    "    scores['depth'].append(depth)\n",
    "    scores['train'].append(train_score)\n",
    "    scores['test'].append(test_score)\n",
    "\n",
    "# Create a dataframe from the scores dictionary and\n",
    "# set the index to depth\n",
    "scores_df = pd.DataFrame(scores).set_index('depth')\n",
    "\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scores dataframe with the plot method\n",
    "scores_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest model\n",
    "classifier = RandomForestClassifier(random_state=13, max_depth=3, n_estimators=100)\n",
    "\n",
    "# Fit (train) or model using the training data\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Calculate the accuracy of the model on the testing data\n",
    "classifier.score(X_test_scaled, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of the model on the training data\n",
    "classifier.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model's balanced accuracy on the test set\n",
    "\n",
    "y_test_pred = classifier.predict(X_test_scaled)\n",
    "print(balanced_accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model does well at identifying when an object is not dangerous, but it does relatively poorly when trying to identify dangerous objects. This is a problem! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the model's confusion matrix\n",
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuning: Should do some overfitting and balanced_score testing\n",
    "# Import RandomOverSampler from imblearn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Instantiate the RandomOverSampler instance\n",
    "random_oversampler = RandomOverSampler(random_state=1)\n",
    "\n",
    "# Fit the data to the model\n",
    "X_resampled, y_resampled = random_oversampler.fit_resample(\n",
    "                                                X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count distinct values\n",
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RandomForestClassifier instance and fit it to the resampled data\n",
    "resampled_model = RandomForestClassifier(random_state=13, max_depth=3, n_estimators=100)\n",
    "resampled_model.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for testing features\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "y_pred_resampled = resampled_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification reports for the two models\n",
    "original_classification_report = classification_report(y_test, y_pred)\n",
    "print(original_classification_report)\n",
    "print('----------')\n",
    "print('Resampled using Random Oversampler:')\n",
    "print(classification_report(y_test, y_pred_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SMOTE from imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Instantiate the SMOTE instance \n",
    "# Set the sampling_strategy parameter equal to auto\n",
    "smote_sampler = SMOTE(random_state=1, sampling_strategy='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the training data to the smote_sampler model\n",
    "X_resampled_smote, y_resampled_smote = smote_sampler.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count distinct values for the resampled target data\n",
    "y_resampled_smote.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new RandomForestClassier model \n",
    "smote_model = RandomForestClassifier()\n",
    "\n",
    "# Fit the resampled data to the new model\n",
    "smote_model.fit(X_resampled_smote, y_resampled_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for resampled testing features\n",
    "smote_y_pred = smote_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification reports\n",
    "print(f\"Classification Report - Original Data\")\n",
    "print(original_classification_report)\n",
    "print(\"---------\")\n",
    "print(f\"Classification Report - Resampled Data - SMOTE\")\n",
    "print(classification_report(y_test, smote_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance is improving, but not enough. Let's try adding more data from the NASA API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the additional data file from JSON into a dataframe\n",
    "path = 'resources/additional_neo_data.json'\n",
    "df_additional = pd.read_json(path)\n",
    "\n",
    "# Count the instances of is_potentially_hazardous == 1\n",
    "df_additional['is_potentially_hazardous'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe of the minority class\n",
    "minority_class_df = df_additional[df_additional['is_potentially_hazardous'] == 1]\n",
    "\n",
    "minority_class_df = remove_unnecessary_columns(minority_class_df)\n",
    "minority_class_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the minority class dataframe with the base dataframe\n",
    "combined_df = pd.concat([base_df, minority_class_df], ignore_index=True)\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the value counts of the target column\n",
    "combined_df['is_potentially_hazardous'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new features set and drop the target column\n",
    "X = get_features_set(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target vector\n",
    "y = combined_df['is_potentially_hazardous'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale with Standard Scaler\n",
    "standard_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = standard_scaler.transform(X_train)\n",
    "\n",
    "# Scale the testing dataset\n",
    "X_test_scaled = standard_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loop to vary the max_depth parameter\n",
    "# Make sure to record the train and test scores \n",
    "# for each pass.\n",
    "\n",
    "# Depths should span from 1 up to 20 in steps of 2\n",
    "depths = range(1, 10, 1)\n",
    "\n",
    "# The scores dataframe will hold depths and scores\n",
    "# to make plotting easy\n",
    "scores = {'train': [], 'test': [], 'depth': []}\n",
    "\n",
    "# Loop through each depth (this will take time to run)\n",
    "for depth in depths:\n",
    "    clf = RandomForestClassifier(max_depth=depth)\n",
    "    clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    train_score = clf.score(X_train_scaled, y_train)\n",
    "    test_score = clf.score(X_test_scaled, y_test)\n",
    "\n",
    "    scores['depth'].append(depth)\n",
    "    scores['train'].append(train_score)\n",
    "    scores['test'].append(test_score)\n",
    "\n",
    "# Create a dataframe from the scores dictionary and\n",
    "# set the index to depth\n",
    "scores_df = pd.DataFrame(scores).set_index('depth')\n",
    "\n",
    "scores_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest model\n",
    "classifier = RandomForestClassifier(random_state=13, max_depth=3, n_estimators=100)\n",
    "\n",
    "# Fit (train) or model using the training data\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Calculate the accuracy of the model on the testing data\n",
    "print(f\"Testing data accuracy score: {classifier.score(X_test_scaled, y_test)}\")\n",
    "\n",
    "# Calculate the accuracy of the model on the training data\n",
    "print(f\"Training data accuracy score: {classifier.score(X_train_scaled, y_train)}\")\n",
    "\n",
    "# Check the model's balanced accuracy on the test set\n",
    "y_test_pred = classifier.predict(X_test_scaled)\n",
    "print(f\"Balanced Accuracy Score: {balanced_accuracy_score(y_test, y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for testing features\n",
    "y_pred = classifier.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification reports for the two models\n",
    "print(\"Classification Report - Original Data\")\n",
    "print(original_classification_report)\n",
    "print('----------------')\n",
    "print('Classification Report: New Data Added')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see if we can get precision and f1-score up a bit more by trying to use SMOTE with the new data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_smote_models(X_train_scaled, y_train, X_test_scaled, y_test, original_classification_report):\n",
    "    # Instantiate the SMOTE instance \n",
    "    # Set the sampling_strategy parameter equal to auto\n",
    "    smote_sampler = SMOTE(random_state=1, sampling_strategy='auto')\n",
    "    # Fit the training data to the smote_sampler model\n",
    "    X_resampled_smote, y_resampled_smote = smote_sampler.fit_resample(X_train_scaled, y_train)\n",
    "    # Instantiate a new RandomForestClassier model \n",
    "    smote_model = RandomForestClassifier()\n",
    "\n",
    "    # Fit the resampled data to the new model\n",
    "    smote_model.fit(X_resampled_smote, y_resampled_smote)\n",
    "    # Predict labels for resampled testing features\n",
    "    smote_y_pred = smote_model.predict(X_test_scaled)\n",
    "    # Calculate the accuracy of the model on the testing data\n",
    "    print(f\"Testing data accuracy score: {smote_model.score(X_test_scaled, y_test)}\")\n",
    "\n",
    "    # Calculate the accuracy of the model on the training data\n",
    "    print(f\"Training data accuracy score: {smote_model.score(X_resampled_smote, y_resampled_smote)}\")\n",
    "\n",
    "    # Check the model's balanced accuracy on the test set\n",
    "    print(f\"Balanced Accuracy Score: {balanced_accuracy_score(y_test, smote_y_pred)}\")\n",
    "    print('---------')\n",
    "    print(f\"Classification Report - Original Data\")\n",
    "    print(original_classification_report)\n",
    "    print(\"---------\")\n",
    "    print(f\"Classification Report - Resampled and Added Data - SMOTE\")\n",
    "    print(classification_report(y_test, smote_y_pred))\n",
    "    return smote_model\n",
    "\n",
    "generate_smote_models(X_train_scaled, y_train, X_test_scaled, y_test, original_classification_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
